{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See NLP_vectors_1 for more info about thought process and how the corpus was created from the vectors available via NCBI. Notebook was separated here because the vector sequences take ~3 hrs to collect via the api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from Bio import Entrez\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "import pickle\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('corpus.csv')\n",
    "corpus = list(corpus['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train W2V I have to 'tokenize' the corpus, or divide up the sequences into 'words' (k-mers). Here I had to make a decision on what length I wanted, or what length was computationally possible. The k-mers have to be subdivided on each reading frame, therefore the number of 'sequences' after tokenization will be [n * k-mer length].\n",
    "\n",
    "I initially tried 20 bp k-mers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sequence: 20384 of 20384, or 99.9951% done"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "kmer_len = 20\n",
    "\n",
    "for ind,sequence in enumerate(corpus):\n",
    "    print('\\r' + 'Tokenizing sequence: ' + str(ind+1) + ' of ' + str(len(corpus)) + \n",
    "                  ', or ' + str(round((ind/len(corpus))*100, 4)) + '% done', end='')\n",
    "    for j in range(kmer_len):\n",
    "        counter = 0\n",
    "        try:\n",
    "            tokenized_sequence = []\n",
    "            while counter+kmer_len < len(sequence):\n",
    "                tokenized_sequence.append(str(sequence[j+counter:j+counter+kmer_len]))\n",
    "                counter += kmer_len\n",
    "            tokenized_corpus.append(tokenized_sequence)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing the file takes a long time, also, the resulting file is 22.6GB. Omit unless you want to create a huge file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"tokenized_corpus.csv\", \"w\", newline=\"\") as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    writer.writerows(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where the model is trained. Parameters can be tuned. Initially used 10-dim vectors and window size of 5 (default is 2). Thought that a larger window could account for restriction sites in each plasmid sequence (are they always in the same order?). Uses skip-gram algorithm for training. Maybe this also needs a larger vector size?\n",
    "\n",
    "***LONG TIME TRAINING D:***\n",
    "\n",
    "Source: https://arxiv.org/pdf/1301.3781.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(tokenized_corpus, size=10, window=5, min_count=90, sg=1)\n",
    "pickle.dump(model, open(\"w5_model.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some info about the model that was trained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182427"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of k-mers present in training set\n",
    "model_vectors = model.wv\n",
    "len(model_vectors.vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
