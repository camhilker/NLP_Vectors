{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec to classify metagenome samples by vector presence:\n",
    "\n",
    "Use Word2Vec method that was used for microbiome samples. Create corpus that is a collection of vector sequences from either Addgene or NCBI. Test on one metagenomic sample (desert) to see if clusters form based on presence of vectors. \n",
    "\n",
    "Start with a k-mer length of 6 since RIs seem to be 6bp. At this time, I do not think that taking into account MCS would make a difference. (because of the nature of Word2Vec, n-grams, etc.) How would W2V act (what does it tell us) when extending the k-mer length to encompass all/most of the MCS? Think more about this...\n",
    "\n",
    "**To try:**\n",
    "\n",
    "See if a longer k-mer length has a better effect (if computationally possible).\n",
    "\n",
    "Expand to more than one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from Bio import Entrez\n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was extracted from the Genome database on NCBI, only (all available) plasmid sequences were selected. This included 20384 plasmids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_metadata = pd.read_csv('all_ncbi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripping the accession numbers from the 'Replicons' column. Only take the first accession number if there is more than one listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions = [vector_metadata['Replicons'][i].partition(':')[2] for i,n in enumerate(vector_metadata['Replicons'])]\n",
    "\n",
    "for i,n in enumerate(accessions):\n",
    "    if '/' in n:\n",
    "        accessions[i] = accessions[i].partition('/')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch plasmid sequence for each accession number and deposit into corpus for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 20383 of 20384, or 99.9951% done"
     ]
    }
   ],
   "source": [
    "Entrez.email = 'camelliahilker@gmail.com'\n",
    "corpus = []\n",
    "\n",
    "for ind,accession in enumerate(accessions):\n",
    "    try:\n",
    "        try:\n",
    "            print('\\r' + 'Fetching ' + str(ind+1) + ' of ' + str(len(accessions)) + \n",
    "                  ', or ' + str(round((ind/len(accessions))*100, 4)) + '% done', end='')\n",
    "\n",
    "            handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"fasta\", retmode=\"text\")\n",
    "\n",
    "            sequence = ''\n",
    "            for line in handle:\n",
    "                sequence += handle.readline().strip()\n",
    "\n",
    "            corpus.append(sequence)\n",
    "\n",
    "        except:\n",
    "            #pause so that API doesn't time out\n",
    "            sleep(5)\n",
    "\n",
    "            print('\\r' + 'Fetching ' + str(ind+1) + ' of ' + str(len(accessions)) + \n",
    "                  ', or ' + str(round((ind/len(accessions))*100, 4)) + '% done', end='')\n",
    "\n",
    "            handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"fasta\", retmode=\"text\")\n",
    "\n",
    "            sequence = ''\n",
    "            for line in handle:\n",
    "                sequence += handle.readline().strip()\n",
    "\n",
    "            corpus.append(sequence)\n",
    "    except:\n",
    "        #pause so that API doesn't time out\n",
    "        sleep(10)\n",
    "\n",
    "        print('\\r' + 'Fetching ' + str(ind+1) + ' of ' + str(len(accessions)) + \n",
    "                  ', or ' + str(round((ind/len(accessions))*100, 4)) + '% done', end='')\n",
    "\n",
    "        handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"fasta\", retmode=\"text\")\n",
    "\n",
    "        sequence = ''\n",
    "        for line in handle:\n",
    "            sequence += handle.readline().strip()\n",
    "\n",
    "        corpus.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_csv = pd.DataFrame(corpus)\n",
    "corpus_csv.to_csv('corpus.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
